{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Fall 2021: Final Coding Exam\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This practicum is due on Moodle by **11:59 PM on Sunday December 12**.  Your solutions to theoretical questions should be done in Markdown/LateX directly below the associated question. Your solutions to computational questions should include any relevant Python code, as well as results and any written commentary.\n",
    "\n",
    "**The rules:**\n",
    "\n",
    "1. Choose any TWO of the following three problems to submit. Note the problems that you are submitting below. If you do not choose two problems, the graders will grade the first 2 problems by default. The graders WILL NOT grade all three problems and pick the 2 highest scores. So it is your responsibility to clearly indicate the TWO problems you are choosing.\n",
    "\n",
    "**I choose Problem X1 and Problem X2**\n",
    "\n",
    "1. All work, code and analysis must be **your own**.\n",
    "1. You may use your course notes, posted lecture slides, textbook, in-class notebooks and homework solutions as resources.  You may also search online for answers to general knowledge questions, like the form of a probability distribution function, or how to perform a particular operation in Python.\n",
    "1. You may **not** post to message boards or other online resources asking for help.\n",
    "1. **You may not collaborate with classmates or anyone else.**\n",
    "1. This is meant to be like a coding portion of your final exam. So, I will be much less helpful than I typically am with homework. For example, I will not check answers, help debug your code, and so on.\n",
    "1. If you have a question, post it first as a **private** Piazza message. If I decide that it is appropriate for the entire class, then I will make it a public post (and anonymous).\n",
    "1. If something is left open-ended, it is probably because I intend for you to code it up however you want, and only care about the plots/analysis I see at the end. Feel free to ask clarifying questions though.\n",
    "\n",
    "Violation of these rules will result in an **F** and a trip to the Honor Code council.\n",
    "\n",
    "---\n",
    "**By writing your name below, you agree to abide by these rules:**\n",
    "\n",
    "**Your name:** Sarah Litz\n",
    "\n",
    "---\n",
    "\n",
    "I chose problems 1 and 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# added packages\n",
    "import heapq\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "## [50 pts] Problem 1:  Route-finding\n",
    "\n",
    "Consider the map of the area to the west of the Engineering Center given below, with a fairly coarse Cartesian grid superimposed.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/engineering_center_grid_zoom.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "The green square at $(x,y)=(1,15)$ is the starting location, and you would like to walk from there to the yellow square at $(25,9)$ with the **shortest total path length**. The filled-in blue squares are obstacles, and you cannot walk through those locations.  You also cannot walk outside of this grid.\n",
    "\n",
    "Legal moves in the North/South/East/West directions have a step cost of 1. Moves in the diagonal direction (for example, from $(1,15)$ to $(2,14)$) are allowed, but they have a step cost of $\\sqrt{2}$. \n",
    "\n",
    "Of course, you can probably do this problem (and likely have to some degree, in your head) without a search algorithm. But that will hopefully provide a useful \"sanity check\" for your answer.\n",
    "\n",
    "#### Part A\n",
    "Write a function `adjacent_states(state)`:\n",
    "* takes a single argument `state`, which is a tuple representing a valid state in this state space\n",
    "* returns in some form the states reachable from `state` and the step costs. How exactly you do this is up to you. One possible format for what this function returns is a dictionary with the keys being the tuple locations and the values of the keys being the step costs. E.g: adjacent_states((1,1)) =  $\\{(2,1):1, (2,2):1.414\\}$\n",
    "\n",
    "Print to the screen the output for `adjacent_states((1,15))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,15) Output:\n",
      " {(1, 16): 1, (2, 15): 1, (2, 16): 1.4142135623730951, (2, 14): 1.4142135623730951}\n"
     ]
    }
   ],
   "source": [
    "## Adjacent States ## \n",
    "# Returns a dictionary where they (key=(tuple representing x,y positioning):value=step cost)\n",
    "        \n",
    "\n",
    "def adjacent_states(state): \n",
    "    grid = Grid() # grab grid info \n",
    "    # Legal Moves: \n",
    "        # N,S,E,W with step cost of 1 \n",
    "        # Diagonal move with step cost of sqrt(2)  \n",
    "    # Illegal Moves: must stay w/in the board boundaries, and cannot go outside of the grid\n",
    "    \n",
    "    possible_moves = {} \n",
    "    # N/E/S/W Moves \n",
    "    north = (state[0], state[1]+1) # North\n",
    "    northeast = (state[0]+1,state[1]+1)\n",
    "    east = (state[0]+1, state[1]) \n",
    "    southeast = (state[0]+1, state[1]-1)\n",
    "    south = (state[0], state[1]-1)\n",
    "    southwest = (state[0]-1,state[1]-1)\n",
    "    west = (state[0]-1, state[1]) \n",
    "    northwest = (state[0]-1,state[1]+1)\n",
    "    \n",
    "    cardinals = (north,east,south,west)\n",
    "    diagonals = (northeast,southeast,southwest,northwest)\n",
    "    for state in cardinals: \n",
    "        if state in grid.valid_states:\n",
    "            possible_moves[state] = 1 # step cost of 1 \n",
    "    for state in diagonals: \n",
    "        if state in grid.valid_states: \n",
    "            possible_moves[state] = math.sqrt(2)\n",
    "    \n",
    "    return possible_moves\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## Initialize the 16 x 25 Grid ## \n",
    "class Grid: \n",
    "    \n",
    "    nrow = 16 \n",
    "    ncol = 25 \n",
    "    start_state = (1,15)\n",
    "    goal_state = (25,9)\n",
    "\n",
    "    ## Blue (Illegal) States ## \n",
    "    blue_states = [] # illegal moves\n",
    "    for c in range(5,23+1): \n",
    "        # Row 1 blue squares\n",
    "        r = 1  \n",
    "        blue_states.append((c,r)) \n",
    "\n",
    "    r_min = 2\n",
    "    r_max = 14\n",
    "    for c in range(1,4+1): \n",
    "        for r in range(r_min,r_max+1): \n",
    "            blue_states.append((c,r))\n",
    "        r_min = r_min + 1 # update the range for the row vals\n",
    "        r_max = r_max -1 \n",
    "\n",
    "    # awkard blue square thing in the middle\n",
    "    for r in range(9,12+1): \n",
    "        c = 10 \n",
    "        blue_states.append((c,r))\n",
    "    for c in range(11,13+1):\n",
    "        for r in range(9,14+1): \n",
    "            blue_states.append((c,r))\n",
    "    for r in range(11,14+1): \n",
    "        c = 14 \n",
    "        blue_states.append((c,r))\n",
    "\n",
    "    for c in range(21,25+1): \n",
    "        for r in range(11,16+1): \n",
    "            blue_states.append((c,r))\n",
    "    \n",
    "    \n",
    "    valid_states = [] \n",
    "    for c in range(1,25+1): \n",
    "        for r in range(1,16+1): \n",
    "            if (c,r) not in blue_states: \n",
    "                valid_states.append((c,r))\n",
    "\n",
    "    all_states = valid_states + blue_states            \n",
    "                \n",
    "## Get Adjacent States ## \n",
    "# adjacent_states((1,1))\n",
    "print(\"(1,15) Output:\\n\", adjacent_states((1,15)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "Three candidate heuristic functions might be:\n",
    "1. `heuristic_cols(state, goal)` = number of columns between the argument `state` and the `goal`\n",
    "1. `heuristic_rows(state, goal)` = number of rows between the argument `state` and the `goal`\n",
    "1. `heuristic_eucl(state, goal)` = Euclidean distance between the argument `state` and the `goal`\n",
    "\n",
    "Write a function `heuristic_max(state, goal)` that returns the maximum of all three of these heuristic functions for a given `state` and `goal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.73863375370596\n"
     ]
    }
   ],
   "source": [
    "def heuristic_cols(state, goal): \n",
    "    return goal[0] - state[0]\n",
    "def heuristic_rows(state, goal): \n",
    "    return abs(goal[1] - state[1])\n",
    "def heuristic_eucl(state, goal): \n",
    "    return math.sqrt((goal[0] - state[0])**2 + (goal[1] - state[1])**2)\n",
    "\n",
    "    \n",
    "def heuristic_max(state,goal): \n",
    "    return max(heuristic_cols(state,goal), \n",
    "        heuristic_rows(state,goal), \n",
    "        heuristic_eucl(state,goal))\n",
    "\n",
    "    \n",
    "grid = Grid() \n",
    "maxh = heuristic_max(grid.start_state, grid.goal_state)\n",
    "print(maxh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "Is the Manhattan distance an admissible heuristic function for this problem?  Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manhattan distance is not an admissible heuristic for this problem because it could potentially overestimate the cost to reach the goal, since it would always calculate the path using a N/E/S/W move, when making a diagonal move might actually be the cheaper option to reach the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "Use A\\* search and the `heuristic_max` heuristic to find the shortest path from the initial state at $(1,15)$ to the goal state at $(25,9)$. Your search **should not** build up the entire state space graph in memory. Instead, use the `adjacent_states` function from Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 15), (2, 15), (3, 15), (4, 15), (5, 15), (6, 15), (7, 15), (8, 15), (9, 15), (10, 15), (11, 15), (12, 15), (13, 15), (14, 15), (15, 14), (16, 13), (17, 13), (18, 12), (19, 12), (20, 11), (21, 10), (22, 10), (23, 10), (24, 9), (25, 9)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here. You may use our class lecture notebooks, your old homework, and/or the homework solutions \n",
    "# that are posted on our Canvas page.\n",
    "\n",
    "#### Path and Path Cost functions from HW2 ####\n",
    "def path(previous, s): \n",
    "    '''\n",
    "    `previous` is a dictionary chaining together the predecessor state that led to each state\n",
    "    `s` will be None for the initial state\n",
    "    otherwise, start from the last state `s` and recursively trace `previous` back to the initial state,\n",
    "    constructing a list of states visited as we go\n",
    "    '''\n",
    "    if s is None:\n",
    "        return []\n",
    "    else:\n",
    "        return path(previous, previous[s])+[s]\n",
    "def pathcost(path):\n",
    "    '''\n",
    "    add up the step costs along a path, which is assumed to be a list output from the `path` function above\n",
    "    '''\n",
    "    cost = 0.0\n",
    "    for s in range(len(path)-1): \n",
    "        state = path[s]\n",
    "        adj_states = adjacent_states(state) \n",
    "        # print(state)\n",
    "        nxt_state = path[s+1] # get key value of the next state we want\n",
    "        nxt_state_cost = adj_states[nxt_state] # grab the stepcost value for the key=\"nxt_state\"\n",
    "        cost = cost + nxt_state_cost\n",
    "    return cost \n",
    "        \n",
    "\n",
    "\n",
    "#### Frontier Priority Queue #### \n",
    "class Frontier_PQ:\n",
    "    ''' frontier class for uniform search, ordered by path cost '''\n",
    "    \n",
    "    def __init__(self, start, cost):\n",
    "        self.states = {}\n",
    "        self.q = []\n",
    "        self.add(start, cost)\n",
    "        \n",
    "    def add(self, state, cost):\n",
    "        ''' push the new state and cost to get there onto the heap'''\n",
    "        heapq.heappush(self.q, (cost, state))\n",
    "        self.states[state] = cost\n",
    "\n",
    "    def pop(self):\n",
    "        (cost, state) = heapq.heappop(self.q)  # get cost of getting to explored state\n",
    "        self.states.pop(state)    # and remove from frontier\n",
    "        return (cost, state)\n",
    "\n",
    "    def replace(self, state, cost):\n",
    "        ''' found a cheaper route to `state`, replacing old cost with new `cost` '''\n",
    "        self.states[state] = cost\n",
    "        for i, (oldcost, oldstate) in enumerate(self.q):\n",
    "            if oldstate==state and oldcost > cost:\n",
    "                self.q[i] = (cost, state)\n",
    "                heapq._siftdown(self.q, 0, i) # now i is posisbly out of order; restore\n",
    "        return\n",
    "    \n",
    "def astar_search(grid, heuristic): \n",
    "    \n",
    "    start = grid.start_state \n",
    "    goal = grid.goal_state \n",
    "    \n",
    "    frontier = Frontier_PQ(start, 0) \n",
    "    previous = {start: None} \n",
    "    explored = {} \n",
    "    while frontier: \n",
    "        state = frontier.pop() \n",
    "        # print('Current State:', state)\n",
    "        \n",
    "        ## Goal state reached ## \n",
    "        if state[1] == goal: \n",
    "            return path(previous, state[1])\n",
    "        \n",
    "        \n",
    "        ## Otherwise, check all adjacent states; add/update when needed. \n",
    "        adj_dict = adjacent_states(state[1])\n",
    "        explored[state[1]] = pathcost(path(previous, state[1]))\n",
    "        # print('Explored:', explored)\n",
    "        # print('Adjacent States:', adjacent_states(state[1]))\n",
    "\n",
    "        \n",
    "        for adj_state in adj_dict: \n",
    "            adj_step_cost = adj_dict[adj_state]\n",
    "            # print('adj state:', adj_state, adj_dict[adj_state])\n",
    "            newcost = explored[state[1]] + adj_step_cost + heuristic(adj_state, goal) # path-cost + heursitic-val \n",
    "            \n",
    "            if (adj_state not in explored) and (adj_state not in frontier.states): \n",
    "                \n",
    "                frontier.add(adj_state, newcost)\n",
    "                previous[adj_state] = state[1]\n",
    "            \n",
    "            elif (adj_state in frontier.states) and (frontier.states[adj_state] > newcost): \n",
    "                \n",
    "                frontier.replace(adj_state, newcost)\n",
    "                previous[adj_state] = state[1]\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "path = astar_search(grid, heuristic_max)\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "Make a figure depicting the optimal route from the initial state to the goal, similarly to how you depicted the maze solution in Homework 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]]\n",
      "(16, 25)\n",
      "25 16\n",
      "[(1, 15), (2, 15), (3, 15), (4, 15), (5, 15), (6, 15), (7, 15), (8, 15), (9, 15), (10, 15), (11, 15), (12, 15), (13, 15), (14, 15), (15, 14), (16, 13), (17, 13), (18, 12), (19, 12), (20, 11), (21, 10), (22, 10), (23, 10), (24, 9), (25, 9)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def grid_to_array(grid): \n",
    "    \n",
    "    # initialize an array using np.array()\n",
    "    # Goal: np.array([\n",
    "    #                [1,1,1,1,1,1,1,1], \n",
    "    #                [1,1,1,1,1,0,1,1] \n",
    "    #               ])\n",
    "    # where the 0's represent the blue squares, and 1's represent the valid squares \n",
    "    \n",
    "    graph = []\n",
    "    for y in range(1, grid.nrow+1): \n",
    "        col = [] \n",
    "        for x in range(1, grid.ncol+1): \n",
    "            if (x,y) in grid.valid_states: \n",
    "                col.append(1)\n",
    "            else: \n",
    "                col.append(0)\n",
    "        graph.append(col)\n",
    "    return np.array(graph)\n",
    "\n",
    "            \n",
    "def visualize_route(grid, path): \n",
    "    \n",
    "    cmap = colors.ListedColormap(['coral','slategray'])\n",
    "    \n",
    "    graph = grid_to_array(grid)\n",
    "    print(graph)\n",
    "    print(graph.shape)\n",
    "    print(grid.ncol, grid.nrow)\n",
    "    '''x = (np.arange(grid.ncol + 1))\n",
    "    # x = np.arange(0,grid.ncol+1,1)\n",
    "    y = np.arange(grid.nrow+1)\n",
    "    # y = np.arange(0,grid.nrow+1,1)\n",
    "    \n",
    "    print(x), print(y)\n",
    "    \n",
    "    #z = np.meshgrid(x,y)\n",
    "    # z = np.array(x,y)\n",
    "    #z = zip(x,y)\n",
    "    #print(z)\n",
    "    # print(z.ndim)'''\n",
    "    \n",
    "    #nrow, ncol = graph.shape\n",
    "    \n",
    "    fig, ax = plt.subplots() \n",
    "    ax.imshow(graph, cmap=cmap, origin='lower', extent=[.5,25.5,.5,16.5])\n",
    "    \n",
    "    ax.grid(which='major', axis='both',linestyle='-', color='k')\n",
    "    ax.set_xticks(np.arange(1.5,grid.ncol,1))\n",
    "    ax.set_yticks(np.arange(1.5,grid.nrow,1))\n",
    "    ax.xaxis.set_ticklabels([])\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Solution Path\n",
    "    if path:\n",
    "        print(path)\n",
    "        for p in range(len(path)-1):\n",
    "            point = path[p]\n",
    "            nextpoint = path[p+1]\n",
    "            plt.plot([point[0],nextpoint[0]], [point[1],nextpoint[1]], c='black')\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "visualize_route(grid, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "## [50 pts] Problem 2:  Reinforcement learning\n",
    "\n",
    "Consider a **cube** state space defined by $0 \\le x, y, z \\le L$. Suppose you are piloting/programming a drone to learn how to land on a platform at the center of the $z=0$ surface (the bottom). Some assumptions:\n",
    "* In this discrete world, if I say the drone is at $(x,y,z)$ I mean that it is in the box centered at $(x,y,z)$. And there are boxes (states) centered at $(x,y,z)$ for all $0 \\le x,y,z \\le L$. Each state is a 1 unit cube. So when $L=2$ (for example), there are cubes centered at each $x=0,1,2$, $y=0,1,2$ and so on, for a total state space size of $3^3 = 27$ states.\n",
    "* All of the states with $z=0$ are terminal states.\n",
    "* The state at the center of the bottom of the cubic state space is the landing pad. So, for example, when $L=4$, the landing pad is at $(x,y,z) = (2,2,0)$.\n",
    "* All terminal states ***except*** the landing pad have a reward of -1. The landing pad has a reward of +1.\n",
    "* All non-terminal states have a reward of -0.01.\n",
    "* The drone takes up exactly 1 cubic unit, and begins in a random non-terminal state.\n",
    "* The available actions in non-terminal states include moving exactly 1 unit Up (+z), Down (-z), North (+y), South (-y), East (+x) or West (-x). In a terminal state, the training episode should end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "Write a class `MDPLanding` to represent the Markov decision process for this drone. Include methods for:\n",
    "1. `actions(state)`, which should return a list of all actions available from the given state\n",
    "2. `reward(state)`, which should return the reward for the given state\n",
    "3. `result(state, action)`, which should return the resulting state of doing the given action in the given state\n",
    "\n",
    "and attributes for:\n",
    "1. `states`, which is just a list of all the states in the state space, where each state is represented as an $(x,y,z)$ tuple\n",
    "2. `terminal_states`, a dictionary where keys are the terminal state tuples and the values are the rewards associated with those terminal states\n",
    "3. `default_reward`, which is a scalar for the reward associated with non-terminal states\n",
    "4. `all_actions`, a list of all possible actions (Up, Down, North, South, East, West)\n",
    "5. `discount`, the discount factor (use $\\gamma = 0.999$ for this entire problem)\n",
    "\n",
    "How you feed arguments/information into the class constructor is up to you.\n",
    "\n",
    "Note that actions are *deterministic* here.  The drone does not need to learn transition probabilities for outcomes of particular actions. What the drone does need to learn, however, is where the heck that landing pad is, and how to get there from any initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPLanding: \n",
    "    \n",
    "    def __init__(self, L):\n",
    "        \n",
    "                ## Range of the State Space Cube: 0 <= (x,y,z) <= L ##\n",
    "        self.L = L\n",
    "        \n",
    "        self.states = [(x,y,z) for x in range (0,L+1) for y in range(0,L+1) for z in range(0,L+1)]\n",
    "        \n",
    "        # terminal states: dictionary with terminal state keys and rewards as values\n",
    "        self.terminal_states = { (x,y,0): -1 for x in range(0,L+1) for y in range(0,L+1) } \n",
    "        \n",
    "        self.terminal_states[(L/2,L/2,0)] = 1 # update the center terminal state to be the landing state w/ +1 reward \n",
    "\n",
    "        self.default_reward = -0.01 # Reward for all non-terminal states\n",
    "        \n",
    "        self.all_actions = ['Up','Down','North','East','South','West']\n",
    "        \n",
    "        self.discount = 0.999\n",
    "    \n",
    "    def actions(self, state): \n",
    "        # return list of all actions available from a given state\n",
    "        if state in self.terminal_states: \n",
    "            return [None]\n",
    "        else: \n",
    "            moves = [] \n",
    "            if state[0] > 0: \n",
    "                # can move West \n",
    "                moves.append('W')\n",
    "            if state[0] < self.L: \n",
    "                # can move East \n",
    "                moves.append('E')\n",
    "            if state[1] > 0: \n",
    "                # can move South \n",
    "                moves.append('S')\n",
    "            if state[1] < self.L:\n",
    "                # can move North \n",
    "                moves.append('N')\n",
    "            if state[2] > 0: \n",
    "                # can move Down \n",
    "                moves.append('Down')\n",
    "            if state[2] < self.L:\n",
    "                # can move Up\n",
    "                moves.append('Up')\n",
    "                \n",
    "            return moves\n",
    "                \n",
    "    def reward(self, state): \n",
    "        # return the reward for the given state\n",
    "        # if you are in a terminal state then look up the reward in the terminal states dictionary, otherwise use the default reward\n",
    "        return self.terminal_states[state] if state in self.terminal_states.keys() else self.default_reward\n",
    "    \n",
    "    def result(self, state, action): \n",
    "        # return the resulting state of doing the given action in the given state \n",
    "        \n",
    "        assert action in self.actions(state), 'Error: action needs to be available in that state'\n",
    "        assert state in self.states, 'Error: invalid state'\n",
    "        \n",
    "        if action=='N':\n",
    "            new_state = (state[0], state[1]+1, state[2])\n",
    "            return new_state\n",
    "        elif action=='S':\n",
    "            new_state = (state[0], state[1]-1, state[2])\n",
    "            return new_state\n",
    "        elif action=='E':\n",
    "            new_state = (state[0]+1, state[1], state[2])\n",
    "            return new_state\n",
    "        elif action=='W':\n",
    "            new_state = (state[0]-1, state[1], state[2])\n",
    "            return new_state\n",
    "        elif action=='Up': \n",
    "            new_state = (state[0], state[1], state[2]+1)\n",
    "            return new_state\n",
    "        elif action=='Down': \n",
    "            new_state = (state[0], state[1], state[2]-1)\n",
    "            return new_state\n",
    "        \n",
    "        # return new_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part B\n",
    "Write a function to implement **policy iteration** for this drone landing MDP. Create an MDP environment to represent the $L=4$ case (so 125 total states).\n",
    "\n",
    "Use your function to find an optimal policy for your new MDP environment. Check (by printing to screen) that the policy for the following states are what you expect, and comment on the results:\n",
    "1. $(2,2,1)$\n",
    "1. $(0,2,1)$\n",
    "1. $(2,0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,2,1): Down\n",
      "(0,2,1): E\n",
      "(2,0,1): N\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Deterministic Actions: we can be sure of the outcome of taking some action. Unlike if we are rolling a die, we can not be sure of what number will come up - so die rolling is nondeterministic, aka stochastic\n",
    "    \n",
    "    # Function to calculate the utilities for each state\n",
    "    \n",
    "    ## Utility \n",
    "    # roughly the sum of rewards along the entire sequence of states we've taken \n",
    "    # sum the rewards from each state, and multiply by the discount factor\n",
    "    # we have an INIFINITE horizon; so no reason to behave differently in the same state at different times\n",
    "        # no fixed time limit. Optimal action only depends on the current state. \n",
    "    \n",
    "    ## Discount Factor = preference for immediate vs. future reward\n",
    "    # if discount factor is close to 0, rewards in distant future are insignificant \n",
    "    # if discount factor close to 1, future rewards just as good/relevant as present one \n",
    "\n",
    "                \n",
    "def utility_function(mdp, tol=1e-3): \n",
    "    ''' return '''\n",
    "    \n",
    "\n",
    "    utility = { s: 0 for s in mdp.states }\n",
    "    \n",
    "    for n in range(100): # 100 iterations\n",
    "        \n",
    "        util_old = utility.copy() \n",
    "        max_change = 0\n",
    "        \n",
    "        # For Each State\n",
    "        for s in mdp.states: \n",
    "\n",
    "            best = -999\n",
    "\n",
    "            old_utility = utility.copy()\n",
    "\n",
    "            # for each possible action from the current state s\n",
    "            for move in mdp.actions(s): \n",
    "\n",
    "                if move == None: \n",
    "                    best = 0 # terminal state\n",
    "                    break\n",
    "\n",
    "                succ_state = mdp.result(s, move) # get (x,y,z) coordinates of making given move from state s\n",
    "\n",
    "\n",
    "                # test if we have found a better utility value for current state \n",
    "                best = max(best,utility[succ_state]) # utility of making given move\n",
    "\n",
    "                # update current state's utility \n",
    "                # print(best)\n",
    "            utility[s] = (mdp.reward(s) + mdp.discount*best)\n",
    "            max_change = max(max_change, abs(utility[s]-util_old[s]))\n",
    "        \n",
    "        \n",
    "        #num = tol*(1-mdp.discount)/mdp.discount\n",
    "        #print(f'{max_change}, {num}')\n",
    "        #if ((mdp.discount==1) and max_change < tol) or (max_change < tol*(1-mdp.discount)/mdp.discount): \n",
    "        #    break\n",
    "\n",
    "\n",
    "    return utility\n",
    "\n",
    "def policy_iteration(mdp): \n",
    "    \n",
    "    \n",
    "    # Policy = dictionary with (x,y,z) keys and their policy value as the value\n",
    "    policy = { s: None for s in mdp.states } # initialize their policy vals to None\n",
    "    \n",
    "    utility = utility_function(mdp)\n",
    "    # print(utility)\n",
    "    \n",
    "    for s in mdp.states: \n",
    "        best = (-999, None)\n",
    "        for a in mdp.actions(s): \n",
    "            succ_state = mdp.result(s,a)\n",
    "            if succ_state is None: \n",
    "                break\n",
    "            new_util = utility[succ_state]\n",
    "            if new_util > best[0]: \n",
    "                best = (new_util, a)\n",
    "        policy[s] = best[1]\n",
    "    return policy\n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "cube = MDPLanding(4) \n",
    "#utility = ordinal_utility_function(cube)\n",
    "policy = policy_iteration(cube)\n",
    "\n",
    "print(f'(2,2,1): {policy[(2,2,1)]}')\n",
    "print(f'(0,2,1): {policy[(0,2,1)]}')\n",
    "print(f'(2,0,1): {policy[(2,0,1)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results given make sense because the only time that we would every want to take the action \"Down\" when sitting anywhere on the z=1 plane of the state space, is if the drone is directly above the landing pad. This is executed correctly as only the state (2,2,1) has a policy of taking the \"Down\" action. The other policies shown are correctly navigating along the (x,y) planes to get closer to the center tile without running into a non-landing pad terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "Code up a **Q-learning** agent/algorithm to learn how to land the drone. You can do this however you like, as long as you use the MDP class structure defined above.  \n",
    "\n",
    "Your code should include some kind of a wrapper to run many trials to train the agent and learn the Q values (see Section 22.3 in the textbook - page 803 might be of particular interest).  You also do not need to have a separate function for the actual \"agent\"; your code can just be a \"for\" loop within which you are refining your estimate of the Q values.\n",
    "\n",
    "From each training trial, save the cumulative discounted reward (utility) over the course of that episode. That is, add up all of $\\gamma^t R(s_t)$ where the drone is in state $s_t$ during time step $t$, for the entire sequence. I refer to this as \"cumulative reward\" because we usually refer to \"utility\" as the utility *under an optimal policy*.\n",
    "\n",
    "Some guidelines:\n",
    "* The drone should initialize in a random non-terminal state for each new training episode.\n",
    "* The training episodes should be limited to 50 time steps, even if the drone has not yet landed. If the drone lands (in a terminal state), the training episode is over.\n",
    "* You may use whatever learning rate $\\alpha$ you decide is appropriate, and gives good results.\n",
    "* There are many forms of Q-learning. You can use whatever you would like, subject to the reliability targets in Part D below.\n",
    "* Your code should return:\n",
    "  * The learned Q values associated with each state-action pair.\n",
    "  * The cumulative reward for each training trial. \n",
    "  * Anything else that might be useful in the ensuing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q table and Frequency table \n",
    "mdp = cube\n",
    "Q = {} # Q dict : {(state,action): action-value} \n",
    "frequencies = {}\n",
    "for s in mdp.states:\n",
    "    Q[s] = {}\n",
    "    for a in mdp.actions(s): \n",
    "        Q[s][a] = 0\n",
    "        frequencies[(s,a)] = 0    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 9, 10), (1, 9, 10), (2, 9, 10), (3, 9, 10), (4, 9, 10), (5, 9, 10), (5, 8, 10), (5, 7, 10), (5, 6, 10), (5, 5, 10), (5, 5, 9), (5, 5, 8), (5, 5, 7), (5, 5, 6), (5, 5, 5), (5, 5, 4), (5, 5, 3), (5, 5, 2), (5, 5, 1)]\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.003725490196078432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random \n",
    "\n",
    "def qlearning(mdp, Q, frequencies): \n",
    "    \n",
    "    alpha = 0.7 # learning rate \n",
    "    \n",
    "    policies = policy_iteration(mdp) # returns table of policy values for every state \n",
    "    # print(policies)\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    ## Start State ##\n",
    "    # Initialize the start state of drone with random non-terminal state for new training episode\n",
    "    idx = random.randint(0,len(mdp.states)-1)\n",
    "    while (mdp.states[idx] in mdp.terminal_states): \n",
    "        idx = random.randint(0,len(mdp.states))\n",
    "    current_state = mdp.states[idx] \n",
    "    \n",
    "    path = [current_state]\n",
    "    \n",
    "    for episode in range(51): \n",
    "\n",
    "        ## New Training Episode ## \n",
    "\n",
    "\n",
    "        ## Successor State ## \n",
    "        # choose next action based on policy \n",
    "        action = policies[current_state]\n",
    "        \n",
    "        # set new state based on chosen action \n",
    "        succ_state = mdp.result(current_state,action)\n",
    "        reward = mdp.reward(current_state) ## OR SHOULD THIS BE mdp.reward(current_state)\n",
    "        all_rewards.append(reward)\n",
    "        \n",
    "        # update tracking values \n",
    "        frequencies[(current_state,action)] += 1 \n",
    "        \n",
    "\n",
    "        # print('Current State: ', current_state)\n",
    "        # print('Succ State:', succ_state)\n",
    "        # print(Q)\n",
    "        # print(np.argmax(Q[succ_state])\n",
    "        if(None in Q[succ_state].keys()): \n",
    "            # Terminal State \n",
    "            Q[succ_state][None] = mdp.reward(succ_state)\n",
    "            break\n",
    "        # print(Q[succ_state])\n",
    "        # m = np.argmax(Q[succ_state])\n",
    "        # print(m)\n",
    "        succ_action = 'W'\n",
    "        best_value = Q[succ_state][succ_action]\n",
    "        for a in Q[succ_state].keys(): \n",
    "            if Q[succ_state][a] > best_value: \n",
    "                best_value = Q[succ_state][a]\n",
    "                succ_action = a\n",
    "                # print('SUCC ACTION: ', succ_action)\n",
    "        # succ_action = np.argmax(Q[succ_state])\n",
    "        # print('\\n')\n",
    "        # print('all nxt states: ', Q[succ_state])\n",
    "        # print('max: ', succ_action)\n",
    "        # print('\\n')\n",
    "        \n",
    "        tdnxt = alpha*frequencies[(current_state,action)]*(reward + mdp.discount*Q[succ_state][succ_action])\n",
    "        diff = tdnxt - Q[current_state][action]\n",
    "        Q[current_state][action] = Q[current_state][action] + alpha*diff\n",
    "        \n",
    "        \n",
    "        current_state = succ_state\n",
    "        path.append(current_state)\n",
    "        # all_rewards.append(mdp.reward(current_state))\n",
    "    \n",
    "\n",
    "    # print(Q)\n",
    "    #avg_reward = (all_rewards/49)\n",
    "    return (path, Q, all_rewards)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# print(Q[(5,5,1)])\n",
    "path, qfinal, all_rewards = qlearning(cube, Q, frequencies) \n",
    "# print(qfinal)\n",
    "print(path)\n",
    "print(all_rewards)\n",
    "print('avg reward: ', (sum(all_rewards)/51))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "Initialize the $L=10$ environment (so that the landing pad is at $(5,5,0)$). Run some number of training trials to train the drone.\n",
    "\n",
    "**How do I know if my drone is learned enough?**  If you take the mean cumulative reward across the last 5000 training trials, it should be around 0.80. This means at least about 10,000 (but probably more) training episodes will be necessary. It will take a few seconds on your computer, so start small to test your codes.\n",
    "\n",
    "**Then:** Compute block means of cumulative reward from all of your training trials. Use blocks of 500 training trials. This means you need to create some kind of array-like structure such that its first element is the mean of the first 500 trials' cumulative rewards; its second element is the mean of the 501-1000th trials' cumulative rewards; and so on. Make a plot of the block mean rewards as the training progresses. It should increase from about -0.5 initially to somewhere around +0.8.\n",
    "\n",
    "**And:** Print to the screen the mean of the last 5000 trials' cumulative rewards, to verify that it is indeed about 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDPLanding(10)\n",
    "Q = {} # Q dict : {(state,action): action-value} \n",
    "frequencies = {}\n",
    "for s in mdp.states:\n",
    "    Q[s] = {}\n",
    "    for a in mdp.actions(s): \n",
    "        Q[s][a] = 0\n",
    "        frequencies[(s,a)] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.0013725490196078432\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.003137254901960784\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.003137254901960784\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.000980392156862745\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.002745098039215686\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.002156862745098039\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.00196078431372549\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.002549019607843137\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.002745098039215686\n",
      "[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]\n",
      "avg reward:  -0.00196078431372549\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "averages = []\n",
    "for t in range(50): \n",
    "    path, qfinal, all_rewards = qlearning(cube, Q, frequencies) \n",
    "    # print(qfinal)\n",
    "    # print(path)\n",
    "    print(all_rewards)\n",
    "    print('avg reward: ', (sum(all_rewards)/51))\n",
    "    \n",
    "print(averages)\n",
    "# print(qfinal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "\n",
    "**Question 1:** Why does the cumulative reward start off around -0.5 at the beginning of the training?\n",
    "\n",
    "**Question 2:** Why will it be difficult for us to train the drone to reliably obtain rewards much greater than about 0.8?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## [50 points] Problem 3:  Calibrating a model for global mean sea level changes\n",
    "\n",
    "<img src=\"http://www.anthropocenemagazine.org/wp-content/uploads/2017/05/future-sea-levels.jpg\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    "**Part A:** Load and plot some data.\n",
    "\n",
    "Let's load a couple data sets.  `data_sealevel.csv` is a data set of global mean sea levels, and the other, `data_temperature.csv` is a data set of global mean temperatures. The following bullets discuss the quantities of interest. \n",
    "* `sealevel` will be a list of global mean sea levels (millimeters). This data is found in a column which resides within the `data_sealevel.csv`\n",
    "* `sealevel_sigma` will be a list of the *uncertainty* in global mean sea levels (millimeters). Use the column labeled `uncertainty` within the `data_sealevel.csv` file to obtain this data, and\n",
    "* `temperature` will be a list of global mean temperatures (degrees Celsius). This data is in the `temperature` column in the `data_temperature.csv` file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the suggested code to load in the data files. Feel free to modify these as you wish, but that\n",
    "# is not necessary.\n",
    "\n",
    "year = []\n",
    "sealevel = []\n",
    "sealevel_sigma = []\n",
    "temperature = []\n",
    "\n",
    "dfSealevel = pd.read_csv(\"data_sealevel.csv\")\n",
    "dfTemperature = pd.read_csv(\"data_temperature.csv\")\n",
    "\n",
    "# We aren't doing any heavy-duty stats stuff, so let's just keep what we need as regular lists\n",
    "year = dfSealevel[\"year\"].tolist()\n",
    "sealevel = dfSealevel[\"sealevel\"].tolist()\n",
    "sealevel_sigma = dfSealevel[\"uncertainty\"].tolist()\n",
    "temperature = dfTemperature[\"temperature\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A (i):**\n",
    "\n",
    "- Make three plots for Global mean surface temperature, Sea level (mm), and Sea Level Uncertainty (mm). The x-axis for each of these plots will be the years over which this data was collected. \n",
    "\n",
    "- Plot the data points as a scatter plots, and plot the three plots side-by-side-by-side (one row, three columns of figures). The point here is learn how to customize your figures a bit more, and also because computer screens are (typically) wider than they are tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plotting code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A (ii):** How does the uncertainty in global mean sea levels change as a function of time?  When is the uncertainty the highest?  Give one reason why you think this might be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Part B:**  The \"out-of-box\" sea-level model\n",
    "\n",
    "In your plot from **(a)**, you should see quite an apparent relationship between increasing temperatures and rising sea levels.  Seeems like someone should try to model the relationship between those two, huh?\n",
    "\n",
    "In the helper function, slr, below, a simple model for temperature-driven changes in global mean sea level (GMSL) is defined. This is the model of [Rahmstorf (2007)](http://science.sciencemag.org/content/315/5810/368).\n",
    "\n",
    "The `slr` model takes two parameters, $\\alpha$ and $T_{eq}$, and requires a time series of global mean temperatures: `slr(alpha, Teq, temperature)`.\n",
    "* `alpha` is the sensitivity of sea-level changes to changes in global temperature. The units for $\\alpha$ are millimeters of sea-level changes per year, or mm y$^{-1}$.\n",
    "* `Teq` is the equilibrium global mean temperature, with units of degrees Celsius.\n",
    "* `temperature` is the time series of global mean surface temperatures, assumed to be relative to the 1961-1990 mean.\n",
    "\n",
    "For now, you do not need to worry too much about how this model works.  It is very simple, and widely used, but the point here is that you can plug in a particular set of temperatures (the model **forcing**) and parameters ($\\alpha$ and $T_{eq}$), and out pops a time series of simulated global mean sea levels.\n",
    "\n",
    "**Our goal:**  pick good values for $\\alpha$ and $T_{eq}$, so that when we run the `slr` model using the observations of temperature (which we plotted above), the model output matches well the observations of global mean sea level (which we also plotted above).\n",
    "\n",
    "The whole process of figuring out what these good parameter values are is called **model calibration**, and it's awesome.  Model Calibration is the point of this problem. Let's have a look at why we need to do this in the first place, shall we?\n",
    "\n",
    "The default parameter choices given in the Rahmstorf (2007) paper are $\\alpha=3.4$ mm y$^{-1}$ and $T_{eq} = -0.5\\ ^{\\circ}$C.\n",
    "\n",
    "**Your task for Part B:**\n",
    "\n",
    "Make a plot that contains:\n",
    "* the observed sea level data as scatter points\n",
    "* the modeled sea levels as a line, using the temperature observations from above as the `temperature` input\n",
    "* an appropriate legend and axis labels\n",
    "* $x$ axis is years\n",
    "* $y$ axis is sea level\n",
    "\n",
    "Note that after you run the `slr` model, you will need to **normalize** the output relative to the 1961-1990 reference period.  That is because you are going to compare it against data that is also normalized against this reference period. The `years` that correspond to the model output should be the same as the `years` that correspond to the `temperature` input. Normalizing data can mean several things. Follow the steps outlined below to \"normalize\" the data in the way needed for this problem:\n",
    "- Compute the mean of the output of the slr model for the years from 1961-1990 (inclusive).\n",
    "- Subtract this value from each entry in the \"sealevel\" list (list returned by the slr function)\n",
    "\n",
    "\n",
    "Make sure that you normalize the data prior to plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def slr(alpha, Teq, temperature):\n",
    "    '''sea-level emulator of Rahmstorf 2007 (DOI: 10.1126/science.1135456)\n",
    "    Takes global mean temperature as forcing, and parameters:\n",
    "    alpha = temperature sensitivity of sea level rise, and\n",
    "    Teq   = equilibrium temperature,\n",
    "    and calculates a rise/fall in sea levels, based on whether the temperature\n",
    "    is warmer/cooler than the equilibrium temperature Teq.\n",
    "    Here, we are only worrying about alpha (for now!)'''\n",
    "\n",
    "    n_time = len(temperature)\n",
    "    deltat = 1\n",
    "    sealevel = [0]*n_time\n",
    "    sealevel[0] = -134\n",
    "    for t in range(n_time-1):\n",
    "        sealevel[t+1] = sealevel[t] + deltat*alpha*(temperature[t]-Teq)\n",
    "\n",
    "    return sealevel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your plot above ought to show decent match for the late 1900s, but diverge a bit further back in time.\n",
    "\n",
    "**The point:**  We can do better than this \"out-of-the-box\" version of the Rahmstorf sea level model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C:**   Figuring out our objective function\n",
    "\n",
    "As our **objective function**, we will use the joint likelihood function of the observed sea level data, given the model simulation.  The following is a detailed description of the derivation of the objective funciton for a hill climbing routine. **Note, you do not need to do anything for this part other than to read about the objective function and execute the cell below, then move to part D.**\n",
    "\n",
    "For a single data point in year $i$, $y_i$, with associated uncertainty $\\sigma_i$, we can assume the likelihood for our model simulation in year $i$, $\\eta_i$, follows a normal distribution centered at the data point.  The model simulation is a **deterministic** result of our parameter choices $\\alpha$ and $T_{eq}$, so we write the likelihood as:\n",
    "\n",
    "$$L(y_i \\mid \\alpha, T_{eq}) = \\dfrac{1}{\\sqrt{2 \\pi} \\sigma_i} e^{-\\dfrac{(\\eta_i(\\alpha, T_{eq}) - y_i)^2}{2\\sigma_i^2}}$$\n",
    "\n",
    "But that only uses a single data point.  Let's use all the data!  The **joint likelihood** is the product of all of the likelihoods associated with the individual data points. But that is the product of a lot of numbers that are less than 1, so it will be **tiny**.  Instead, we should try to optimize the **joint log-likelihood**, which is simply the (natural) logarithm of the joint likelihood function.\n",
    "\n",
    "If we assume the observational data ($y_i$) are all independent, then the joint log-likelihood is:\n",
    "\n",
    "$$l(\\mathbf{y} \\mid \\alpha, T_{eq}) = -\\dfrac{N}{2} \\log{(2\\pi)} - \\sum_{i=1}^N \\log{(\\sigma_i)} - \\dfrac{1}{2}\\sum_{i=1}^N \\left( \\dfrac{\\eta_i(\\alpha, T_{eq}) - y_i}{\\sigma_i} \\right)^2$$\n",
    "\n",
    "where, $\\mathbf{y} = [y_1, y_2, \\ldots, y_N]$ is the entire vector (list) of sea level observations, $\\eta(\\alpha, T_{eq}) = [\\eta_1(\\alpha, T_{eq}), \\eta_2(\\alpha, T_{eq}), \\ldots, \\eta_N(\\alpha, T_{eq})]$ is the entire vector (list) of `slr` model output when the parameter values $\\alpha$ and $T_{eq}$ are used, and $N$ is the number of observations we have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining our objective function**\n",
    "\n",
    "Now define a `log_likelihood(parameters, obs_mu, obs_sigma)` function:\n",
    "* `parameters`: argument that is a list of two parameter values, $[\\alpha, T_{eq}]$\n",
    "  * within the likelihood function, you will need to generate the model simulation $\\eta(\\alpha, T_{eq})$ using the input `parameters`, for comparison against the observational data\n",
    "* `obs_temp`: argument that is a time series (list) of observed global mean temperatures, that will be used to run the `slr` model. Provide a default value of `temperature` for this, because we only have one temperature data set to use, and we don't want to keep \n",
    "* `obs_mu`: argument that is a time series (list) of observed values, that will be used for comparison against the `model` output. Provide a default value of `sealevel` here, because we won't be changing the observational data.\n",
    "* `obs_sigma`: argument that is a time series (list) of the corresponding uncertainties in the observational data. Simiarly, provide a default value of `sealevel_sigma` here, so we can avoid the tedious task of sending the data set into this function.\n",
    "* all three of these inputs should be lists, and should be the same length\n",
    "* this routine should return a **single** float number, that is the joint log-likelihood of the given `model` simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the objective function. You will be using this function below when you code up hill-climbing and \n",
    "# simulated annealing routines.\n",
    "\n",
    "def log_likelihood(parameters, obs_temp=temperature, obs_mu=sealevel, obs_sigma=sealevel_sigma):\n",
    "    model = slr(alpha=parameters[0], Teq=parameters[1], temperature=temperature)\n",
    "    \n",
    "    # normalize\n",
    "    reference = (year.index(1961), year.index(1990))\n",
    "    model -= np.mean(model[reference[0]:(reference[1]+1)])\n",
    "\n",
    "    return np.sum([np.log(stats.norm.pdf(x=model, loc=obs_mu, scale=obs_sigma))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D:**  Defining our class structure\n",
    "\n",
    "Now we will apply a hill-climbing algorithm to tune the $\\alpha$ and $T_{eq}$ parameters.\n",
    "\n",
    "Using our in-class lecture notebook on hill-climbing as a guide, do the following:\n",
    "\n",
    "* Define a `State` class, with attributes for the parameter values (which define the state) and the objective function value of that state.\n",
    "* Define a `Problem_hillclimb` **sub-class** of the more general class `Problem`, with:\n",
    "  * attributes for the current `State` (a `State` object), the `objective_function` (the log-likelihood defined above), and `stepsize`. You will need to play around to decide what an appropriate stepsize is. Keep in mind that you may need a different stepsize for each of $\\alpha$ and $T_{eq}$.\n",
    "  * methods for `moves` (return the list of all possible moves from the current state) and `best_move` (return the move that maximizes the objective function).\n",
    "  * the `moves` available should be in proper 2-dimensional space.  Do **not** simply optimize one parameter, keeping the other fixed, then optimize the other parameter, while keeping the first fixed.  (*That method *can* work, but there are some theoretical issues that would need to be tackled, and we are not getting into that here.*) You are allowed to restrict yourself to movements along a grid, as long as you entertain steps in both the $\\alpha$ and the $T_{eq}$ directions.\n",
    "* Define the `hill_climb` algorithm, with any necessary modifications (here, and in the above classes) for the new 2-dimensional state space.\n",
    "  * `hill_climb(problem, n_iter)`:  arguments are a `Problem_hillclimb` object and number of iterations, `n_iter`\n",
    "  * return a `State` that corresponds to the algorithm's guess at a global maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "1. define an initial state object, using the default values from Rahmstorf 2007 as a starting point.\n",
    "2. define a hill-climbing problem object, using this initial state, the log-likelihood objective function, and stepsize(s) of your choosing. (The stepsize(s) may require some playing around to find something you are happy with.)\n",
    "3. ***hill-climb!!!*** Use a number of iterations that you deem appropriate. \n",
    "\n",
    "Play around until you have a simulation that you are happy with.  Then:\n",
    "1. Print to screen the parameter values and corresponding log-likelihood value.\n",
    "2. Compare this calibrated log-likelihood value to the \"out-of-box\" model (above).\n",
    "3. Make a plot of:\n",
    "  * the sea level observations as scatter points\n",
    "  * the uncalibrated model as one line\n",
    "  * the calibrated model as another line\n",
    "  * include axis labels and a legend\n",
    "  \n",
    "**\"Unit tests\":**\n",
    "* As a benchmark, make sure that your log-likelihood is *at least* -500.\n",
    "* Your calibrated (optimized) model simulation should be going straight through the data points.\n",
    "* If this isn't the case, remember to normalize your model against the 1961-1990 reference period!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E:**  Simulated annealing\n",
    "\n",
    "Let's re-calibrate the `slr` model. This time, we will use **simulated annealing**. Again, using our in-class activity as a guide, do the following:\n",
    "\n",
    "* Continue to use your `State` class above.\n",
    "* Define a `Problem_annealing` sub-class of the `Problem` class, with:\n",
    "  * attributes for the current `State` (a `State` object), the `objective_function` (the log-likelihood defined above), and `stepsize`. You will need to play around to decide what an appropriate stepsize is. Keep in mind that you may need a different stepsize for each of $\\alpha$ and $T_{eq}$.\n",
    "  * method for `random_move`, to pick a random move **by drawing from a multivariate normal distribution**.  You should use the `stepsize` attribute as the covariance (width) for this.\n",
    "* Define the `simulated_annealing` algorithm, with any necessary modifications (here, and in the above classes) for the new 2-dimensional state space.\n",
    "  * `simulated_annealing(problem, n_iter)`:  arguments are a `Problem_annealing` object and number of iterations, `n_iter`\n",
    "  * return a `State` that corresponds to the algorithm's guess at a global maximum\n",
    "\n",
    "Subject to the above constraints, you may implement these however you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "1. define an initial state object, using the default values from Rahmstorf 2007 as a starting point.\n",
    "2. define a simulated annealing problem object, using this initial state, the log-likelihood objective function, an appropriate temperature updating schedule and stepsize(s) of your choosing. (The stepsize(s) may require some playing around to find something you are happy with.)\n",
    "  * note that this \"temperature\" is distinct from the actual physical temperature used as input to drive the `slr` model\n",
    "3. ***anneal!!!*** Use a number of iterations that you deem appropriate. \n",
    "\n",
    "Play around until you have a simulation that you are happy with.  Then:\n",
    "1. Print to screen the parameter values and corresponding log-likelihood value.\n",
    "2. Compare this calibrated log-likelihood value to the \"out-of-box\" model (above).\n",
    "3. Make a plot of:\n",
    "  * the sea level observations as scatter points\n",
    "  * the uncalibrated model as one line\n",
    "  * the calibrated model as another line\n",
    "  * include axis labels and a legend\n",
    "  \n",
    "**\"Unit tests\":**  How does your model look when you plot it against the data? If it doesn't look good, then you failed this unit test :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F:**\n",
    "\n",
    "Briefly summarize your findings. Specifically discuss the $\\alpha$ and $T_{eq}$ parameter values you found in **Part D** and **Part E**. How do these compare to the parameters of the model given by Rahmstorf? Did your hill-climbing and/or your simulated annealing programs find a better fit than the Rahmstorf model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
